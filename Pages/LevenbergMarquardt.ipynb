{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levenberg Marquardt\n",
    "\n",
    "### Introduction: \n",
    "\n",
    "The [Levenberg Marquardt](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) algorithm a basic practical algorithm for solving [nonlinear least-sqaure](https://en.wikipedia.org/wiki/Non-linear_least_squares) problems. A better fitting name for this algorithm is damped gauss newton for it combines Gauss-Newton and Gradient Descent. If you have not already, please see my tutorials for [GuassNewton](GaussNewton.ipynb) and [Gradient Descent](GradientDescent.md) for this tutorial builds heavily off of concepts introduced in those algorithms. \n",
    "\n",
    "### Notation:\n",
    "\n",
    "For clarity, this tutorial uses the following notation. Do not be alarmed if you are un-aware of any of the terms described below for they will be introduced in this tutorial. \n",
    "\n",
    "$$ m \\ = \\ number \\ of \\ samples  $$ \n",
    "$$ n \\ = \\ number \\ of \\ parameters $$ \n",
    "$$ \\theta \\ = \\ model \\ parameters $$ \n",
    "$$ r_{m,1} \\ = \\ residual \\ = \\ y - \\hat y $$ \n",
    "$$ J_{m,n} \\ = \\ jacobian \\ =  [ \\nabla r ] $$ \n",
    "$$ p \\ = \\ step \\ \\  update \\ \\ to \\ \\  \\theta  $$ \n",
    "\n",
    "### Theory: \n",
    "\n",
    "While [GuassNewton](GaussNewton.ipynb)(GN) is a basic nonlinear least squares(NLSQ) algorithm, it often fails to converge unless the initial conditions are very close to the correct solution. Likewise, [Gradient Descent](GradientDescent.md)(GD) can be another algorithm to solve NLSQ but it often takes far too many iterations to converge. Consequently, this algorithm attempts to blend those two algorithms together in order to achieve the following goals:  \n",
    "\n",
    "* Robustness against a poor initial guesses\n",
    "* Relatively quick convergence / low total iteration count\n",
    "\n",
    "Practically speaking, Levenberg Marquardt is the most basic ** practical ** algorithm for solving NLSQ since the drawbacks of GN and GD often eliminate their application in industry. As mentioned previously, this algorithm combines GN and GD. Recall the GD and GN steps: \n",
    "\n",
    "$$ p_{GD} = \\alpha \\frac{J^T r}{ || J^T r ||} $$ \n",
    "$$ p_{GN} = -(J^T J)^{-1} J^T r  $$ \n",
    "\n",
    "The idea behind Levenberg Marquardt(LM) is to switch between these two steps based upon the performance of $r_k$ for each $p_k$ step. If the GN step reduce error($ || r || $) it is best to use it and if it increases the error, it is better to take a GD step. Rather than hard switching between these two algorithms, LM introduces a variable $\\lambda$ that is heuristically updated based upon performance. The LM step is as follows: \n",
    "\n",
    "$$ p_{LM} = -(J^T J + \\lambda I)^{-1} J^T r $$\n",
    "\n",
    "The rational for this is if $\\lambda$ is large, the term $ (J^T J + \\lambda I)^{-1} $ simply becomes the identity matrix($I$) multiplied by some small scalar. This is equivalent of the GD step. Likewise, if $\\lambda$ becomes 0, it is obviously the GN step. The next question then becomes how to update $\\lambda$ ? This is done simply adjusting it based upon improvement in reduction in error. If the model error of a given iteration($k$) has decreased, reduce $\\lambda_{k+1} = 0.1 \\cdot \\lambda_k $ and apply the step ($ \\theta_{k+1} = \\theta_k + p_k $). If the model error is increased,  increase lambda ($\\lambda_{k+1} = 10 \\cdot \\lambda_k $) and do not apply the step ($ \\theta_{k+1} = \\theta_k $) . All together the algorithm can be summarized in the following steps: \n",
    "\n",
    "1. Calculate the residual: $r(\\theta_k)$.\n",
    "2. Calculate the jacobian: $J(\\theta_k)$.\n",
    "3. Calculate the LM step $p_k$\n",
    "4. Calculate the new cost $r(\\theta_k + p_k)$\n",
    "5. IF ( $r(\\theta_k + p_k)$ < $r(\\theta_k)$): A, else B: \n",
    "   1. Decrease Lambda: ( $\\lambda_{k+1} = 0.1 \\cdot \\lambda_k $). Update theta: ($ \\theta_{k+1} = \\theta_k + p_k $)\n",
    "   2. Increase Lambda: ($\\lambda_{k+1} = 10 \\cdot \\lambda_k$). Do not update theta:  ($ \\theta_{k+1} = \\theta_k $)\n",
    "6. Determine if algorithm should terminate. \n",
    "\n",
    "Here is matlab/octave code to implement the above algorithm: \n",
    "\n",
    "```matlab \n",
    "function theta = LMS(Rfnc,params,iterations)\n",
    "\n",
    "alpha = 1;\n",
    "theta = params;\n",
    "oldCost = norm(Rfnc(theta));\n",
    "\n",
    "for i =1:iterations;\n",
    "    r = Rfnc(theta);\n",
    "    J = Jf(Rfnc,theta);\n",
    "    p = -pinv(J'*J + alpha*eye(length(params)))*J'*r;\n",
    "    newCost = norm(Rfnc(theta+p));\n",
    "    if(newCost<oldCost)\n",
    "        theta = theta+p;  \n",
    "        oldCost = newCost;\n",
    "        alpha =0.1*alpha;\n",
    "    else\n",
    "        alpha = 10*alpha;\n",
    "    end\n",
    "end\n",
    "\n",
    "end\n",
    "\n",
    "```\n",
    "\n",
    "In this algorithm, `` Rfnc `` is a function pointer to the residual function which should be $r(\\theta) = y - f(\\theta)$. ``Jf `` is a function to calculate the Jacobian. For simplicity in this case, the Jacobian is calculated through finite differences as opposed to the analytical derivative. Naturally this can be more computationally exspensive. The code for ``Jf`` is below.\n",
    "```matlab\n",
    "function J=Jf(fnc,params)\n",
    "\n",
    "eps = 1e-8;\n",
    "x1 = fnc(params);\n",
    "m = size(x1,1);\n",
    "n = length(params);\n",
    "J = zeros(m,n);\n",
    "\n",
    "for i = 1:n\n",
    "    paramCpy = params; \n",
    "    paramCpy(i)= paramCpy(i) + eps;\n",
    "    J(:,i) = (fnc(paramCpy) - x1)/eps;\n",
    "end\n",
    "\n",
    "end\n",
    "```\n",
    "\n",
    "### Example 1: \n",
    "\n",
    "As an example, consider a model of a sinusoid with unkown amplitude and frequency. \n",
    "\n",
    "$$ f(t) = \\theta_1 sin( \\frac{t}{ \\theta_2}) $$ \n",
    "\n",
    "This model is convinently two variables(for plotting purposes) as well as non-linear with respect to the parameters. This non-linearity is apparent when observing the jacobian.  \n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial \\theta} = \\bigg[ sin(\\frac{t}{ \\theta_2}) \\ , \\ -\\frac{\\theta_1}{ \\theta_2^2}  cos( \\frac{t}{\\theta_2})\\bigg] $$\n",
    "\n",
    "This non-linearity / non-convexity further observed in a plot of the cost function: \n",
    "\n",
    "<p align='center'><img src='Images/Nonlinear/GN_3.png'></p>\n",
    "\n",
    "Previously, this problem was evaluted in my seciton on [GuassNewton](GaussNewton.ipynb) and it was stated that the initial guess ($\\theta_0$) must be close to the solution in order for convergence. In contrast, the Levenberg Marquardt algorithm is much more robust. The following is an example with an initial guess of $\\theta_0 = [100;100]$. \n",
    "\n",
    "<p align='center'><img src='Images/Nonlinear/LM_1.png'></p>\n",
    "\n",
    "The following is the code. Note you will need to create the LMS function as the Jf function from the code provided above. \n",
    "\n",
    "```matlab\n",
    "clc\n",
    "t = (-20:0.5:20)';\n",
    "y = 5*sin(-t/5);\n",
    "noise = 1*randn(length(t),1);\n",
    "yM = y + noise; % This is the measurement. \n",
    "iterations = 50; % Fixed iteration length \n",
    "theta = [100;100];\n",
    "rFncs = @(T) yM - (T(1)*sin(-t/T(2))); \n",
    "\n",
    "thetaGN = gaussNewtonNL(rFncs,theta,iterations);\n",
    "thetaLM = LMS(rFncs,theta,iterations);\n",
    "\n",
    "fig1 = figure(1);\n",
    "clf(fig1);\n",
    "hold on\n",
    "scatter(t,yMeasured);\n",
    "plot(t,y)\n",
    "plot(t,thetaGN(1)*sin(-t/thetaGN(2)) )\n",
    "plot(t,thetaLM(1)*sin(-t/thetaLM(2)) )\n",
    "\n",
    "grid on\n",
    "set(gca,'FontSize',10,'FontWeight','bold');\n",
    "set(gcf,'Units','Pixels');\n",
    "set(gcf, 'Position', [2500, 500, 750, 450]);\n",
    "legend('Measured y', 'True y','Gauss Newton','Levenberg Marquardt')\n",
    "title(['NLSQ: \\theta_1 * sin(t / \\theta_2)',' . \\theta_0 = [',num2str(theta(1)),',',num2str(theta(2)),']'])\n",
    "```\n",
    "\n",
    "Of course, this single example does not prove much however it illustrates an example where LM converged where GN did not. To further illustrate this point, consider extending the initial guess through a range of values and take a statistical sampling of convergence. In the following example, $\\theta_2$ is swept from -2000 to 3000. Each experiment was performed 100 times to ensure convergence was not due to noise.   \n",
    "\n",
    "<p align='center'><img src='Images/Nonlinear/LM_2.png'></p>\n",
    "\n",
    "The results rather painfully demonstrate the inadequacy of GN in practical applicaiton. In constrast, in this specific example, LM's initial guess of $\\theta_2$ could be off by 3 orders of magnitude. In practice, how far off the initial guess can saftely be will be dictated by the convexity of the cost plot. None the less, it demonstrates the potential reliablity of LM if the problem is setup efficently and a somewhat intelligent initial guess is made.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
