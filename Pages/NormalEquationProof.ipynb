{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation Proof\n",
    "\n",
    "### Introduction: \n",
    "\n",
    "Many of us have encountered and employed a seemingly magical equation known as the <a href=\"https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)\"><i>Normal Equation</i></a>.  \n",
    "\n",
    "$$ \\huge{ \\theta = (A^T A)^{-1} A^T y } $$\n",
    "\n",
    "This equation gives the optimal model parameters ($\\theta$) to fit a linear model ($A$) to data ($y$). However, after long and repeated use, one may acquire the madness to wonder how it is derived. Thankfully, the derivation is not that complicated and does not require many tricks apart from basic algebra, linear algebra, and calculus. The following is a proof attempts properly show the derivation of the normal equation. Suppose that we have some linear model parameters $ \\theta $ such that when multiplied by our system matrix $ A $ one gets an estimated model output $ \\hat y $ .\n",
    "\n",
    "Our model equation is simply: \n",
    "\n",
    "$$ \\hat y = A \\theta $$\n",
    "\n",
    "Now, we may wish to optimize our model parameters such that the mean squared error is minimized. In other words we wish to find $\\theta$ such that: \n",
    "\n",
    "$$ min( ( \\hat y - y)^2 ) $$\n",
    "\n",
    "For simplicity, we create a vector of this error called the residual vector which is the following: \n",
    "\n",
    "$$ r = \\hat y - y $$ \n",
    "\n",
    "So in other words we wish to minimize:\n",
    "\n",
    "$$ \\large{min_\\theta( ||r||^2 )}  $$\n",
    "\n",
    "Recall that the norm of our error vector can be described as:\n",
    "\n",
    "$$ || r || ^2 = r^T r $$\n",
    "\n",
    "Next, substitute back in the definition of $ r $ and our model to arive at:\n",
    "\n",
    "$$ || r || ^2 = ( A \\theta - y )^T ( A  \\theta - y ) $$ \n",
    "\n",
    "From here apply linear algebra:\n",
    "\n",
    "Apply transpose property: \n",
    "\n",
    "$$ \\dots  = ( \\theta^T A ^T - y^T ) ( A \\theta - y ) $$ \n",
    "\n",
    "Apply foil: \n",
    "\n",
    "$$ \\dots  = \\theta ^T A^T A \\theta  -  \\theta^T A^T y  -  y^T A \\theta + y^T y $$  \n",
    "\n",
    "Apply transpose properties to group terms: \n",
    "\n",
    "$$ \\large{ \\dots  = \\theta ^T A^T A \\theta - 2 y^T A \\theta + y^T  y } $$  \n",
    "\n",
    "Now, to minimize this function, we know that we can take the derivative and set it equal to zero. In other words, we wish to: \n",
    "\n",
    "$$ \\large{ \\frac{ \\partial || r ||^2 }{ \\partial \\theta }}  \\large{ = 0}$$\n",
    "\n",
    "[Matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus) is required in order to take the derivative of a matrix based system. There is an identity that states:\n",
    "\n",
    "$$  \\large{ \\frac{ \\partial x^T A x }{ \\partial x }} = 2Ax $$ \n",
    "\n",
    "Applying this identity as well as basic calculus, we end up with \n",
    "\n",
    "$$ \\large{ \\frac{ \\partial || r ||^2 }{ \\partial \\theta } =  2 A^T A \\theta - 2 A^T y } = 0 $$\n",
    "\n",
    "Recalling that we wish to set this to zero as well as basic algebra we arrive at: \n",
    "\n",
    "$$ \\large{ A^T A \\theta = A^T y }  $$\n",
    "\n",
    "Next if we left multiply both sides by $ (A^T A)^{-1} $ we end up with:\n",
    "\n",
    "$$ \\large{ \\theta = (A^T A)^{-1} A^T y } $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
