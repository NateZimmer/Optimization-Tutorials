{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gauss Newton\n",
    "\n",
    "Gauss-Newton is an algorithm for solving [non-linear least squares](https://en.wikipedia.org/wiki/Non-linear_least_squares) problems. It is an [SQP](https://en.wikipedia.org/wiki/Sequential_quadratic_programming) method which means it is iterative algorithm and that it opperates on the [QP](https://en.wikipedia.org/wiki/Quadratic_programming) subproblem. \n",
    "\n",
    "## Introduction: \n",
    "\n",
    "Consider that one has a residual function $r(\\theta)$ that represents the error of each data point with the respect to the model parameters($\\theta$). Now this function may be non-linear and its nature may be unpredictable. The idea of Gauss Newton is to approximate this function of unknown characteristics with a quadratic. This quadratic is formulated using the taylor series expansion of a matrix based function. Now, since the residual function represents error, naturally the overall goal is to minimize this function. As we know from basic calculus, taking the derivative of a function and setting it equal to zero will yield an extrema(possible local or global maxima or minima) of that function. \n",
    "\n",
    "### Implementation:\n",
    "\n",
    "The following is the 2 term taylor series expansion of a multiple variable function. \n",
    "\n",
    "$$ T(x) = f(a) + (x-a)^T \\nabla f(a) + \\frac{1}{2!}(x-a)^T (\\nabla^2 f(a))(x-a)   $$\n",
    "\n",
    "Next substitute $a = \\theta-p$ and the expression becomes:\n",
    "\n",
    "$$ T(\\theta + p) = f(\\theta) + (p)^T \\nabla f(\\theta) + \\frac{1}{2!}p^T (\\nabla^2 f(\\theta))( p)   $$\n",
    "\n",
    "Recall, to minimize a function(with respect to a parameter $p$), one takes the derivative(with respect to the parameter $p$) and sets it equal to zero. Finally, one solves for that parameter . In this case, we are solving for the step($p$) that takes the function to a minimum. \n",
    "\n",
    "$$ 0 = \\nabla f(\\theta) + \\nabla^2 f(\\theta) p$$\n",
    "$$ p = -(\\nabla^2 f(\\theta))^{-1} \\nabla f(\\theta)  $$\n",
    "\n",
    "The first and second order derivatives of a residual function can be approximated as follows: \n",
    "\n",
    "$$ \\nabla f = J^T r $$\n",
    "$$ \\nabla^2 f \\approx J^T J $$\n",
    "\n",
    "Which yields the following step:\n",
    "\n",
    "$$ p = -(J^T J)^{-1} J^T r $$\n",
    "$$ \\theta_{k+1} = \\theta_{k} + p $$\n",
    "\n",
    "It follows then if one updates iteratively $ \\theta $ by $p$, one would reach the minimum error. The following is some basic code to implement this concept in matlab/octave: \n",
    "\n",
    "```matlab\n",
    "function theta = gaussNewton(J,y,theta,itLimit)\n",
    "\n",
    "for i = 1:itLimit; \n",
    "    r = y - (-J*theta); % Calculate residual\n",
    "    g = (J')*r;\n",
    "    Hinv = pinv((J')*J);\n",
    "    p = -Hinv*g;\n",
    "    theta = theta + p; % Update Theta\n",
    "end\n",
    "\n",
    "end\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Limitations: \n",
    "\n",
    "It is important to note that this method is intrinsically not perfect because of the following reasons:\n",
    "1. A taylor series expansion is an approximation. It becomes less accurate the further $x$ is away from $a$. \n",
    "2. This method relies on an initial guess of $\\theta_0$. The performance of the algorithm is subject to that initial guess.\n",
    "3. $J^T J$ is an approximation of the hessian and $J^T$ itself is often approximated as well. \n",
    "4. This method is often is operating on nonlinear objective functions which are not convex. \n",
    "\n",
    "Practically speaking these limitations mean the following: \n",
    "1. With a poor initial guess, the algorithm may not converge. \n",
    "2. With a poor initial guess, the algorithm may not reach a desired global optimum. \n",
    "3. With a highly nonlinear function, the algorithm may be incapable of reaching the desired global optimum.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
